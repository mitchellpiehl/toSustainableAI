<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Socially Sustainable AI</title>
    <link rel="icon" href="images/aiIcon.png" type="image/png">
    <link rel="stylesheet" href="normalize.css">
    <link rel="stylesheet" href="lightbox/lightbox.min.css">
    <link rel="stylesheet" href="final.css">
    <script src="jquery-3.7.1.min.js"></script>
    <script src="lightbox/lightbox.min.js"></script>
</head>
<body>
    <div id="whole-wrapper">
        <header class="parallax-intro">
            <h1 class="intro-title">Socially Sustainable Artificial Intelligence</h1>
        </header>

        <nav id="header">
            <div class="hcol1">
                <h1 class="head">Sustainable AI</h1>
            </div>
            <div class="hcol2">
                <label for="menu-toggle" id="mobilenav">&#9776;</label>
                <input type="checkbox" id="menu-toggle">
                <ul id="nav">
                    <li><a href="index.html" class="current">Homepage</a></li>
                    <li><a href="environmental.html">Environmental</a>
                        <ul>
                            <li><a href="optimization.html">Optimization</a></li>
                            <li><a href="efficiency.html">Efficiency</a></li>
                        </ul>
                    </li>
                    <li><a href="social.html">Social</a>
                        <ul>
                            <li><a href="#dangers">AI Dangers</a></li>
                            <li><a href="#solutions">Solutions</a></li>
                            <li><a href="#unknown">Unanswered Questions</a></li>
                        </ul>
                    </li>
                    <li><a href="helping.html">Helping Sustainability</a>
                        <ul>
                            <li><a href="helping.html#supply">Supply Chain</a></li>
                            <li><a href="helping.html#resource">Resource Management</a></li>
                            <li><a href="helping.html#climate">Climate Monitoring</a></li>
                        </ul>
                    </li>
                </ul>
            </div>
        </nav>

        <main id="whole-wrapper">
            <div id="wrapper">

            
            <section id="content" class="grid">
                <div class="gcol1">
                    <a href="images/AIchip.jpg" data-lightbox="image1"><img src="images/AIchip.jpg" alt="AI Chip" class="img"></a>
                </div>
                <div class="gcol2"></div>
                <div class="gcol3">
                    <h1 class="large">Social Risks of Artificial Intelligence</h1>
                    <p>AI has the potential to revolutionize many aspects of modern life—streamlining work, enhancing creativity, and offering personalized assistance. However, its adoption also raises serious societal concerns. From job displacement to intellectual property misuse, it is crucial to critically examine AI's social impact.</p>
                    <p>Continue reading to learn more about the dangers of AI, a couple of important solutions to these dangers, and important unanswered questions revolving around AI.</p>
                </div>
            </section>
        </div>
            <section id="dangers" class="break">
                <div id="content">
                    <h1 class="x-large">The Dangers of AI</h1>
                </div>
            </section>


            <section class="parallax-wrapper5">
                <div class="stuff2">
                <h1 class="parallax-content">Bias and Discrimination</h1>
                <p class="parallax-content">
                    AI systems often reproduce historical and structural biases in training data, leading to skewed outcomes. For example, COMPAS, a criminal risk-assessment tool, was shown to be significantly more likely to falsely flag Black defendants as high risk compared to white defendants—raising concerns about fairness in law enforcement systems.  
                    (<a href="https://en.wikipedia.org/wiki/COMPAS_%28software%29" target="_blank">COMPAS bias findings</a>)
                </p>
                <p class="parallax-content">
                    Language models also harbor covert “dialect prejudice,” penalizing speakers of non-standard English (like African American English) more harshly—for instance, suggesting less prestigious jobs or harsher sentences based solely on speech patterns.  
                    (<a href="https://arxiv.org/abs/2403.00742" target="_blank">Dialect prejudice in language models</a>)
                </p>
                </div>
            </section>

            <section class="parallax-wrapper5">
                <div class="stuff2">
                <h1 class="parallax-content">Content Summarization and Revenue Impact</h1>
                <p class="parallax-content">
                    AI-powered summarization tools can streamline content access but may draw audiences away from original publishers—potentially reducing ad revenue and undermining journalistic sustainability. Conservative estimates suggest significant impacts on digital news ecosystems, though industry-wide studies remain scarce.
                </p>
                </div>
            </section>

            <section class="parallax-wrapper5">
                <div class="stuff2">
                <h1 class="parallax-content">Lack of Explainability</h1>
                <p class="parallax-content">
                    Many modern AI models act as "black boxes," making it difficult for users—even developers—to understand how they arrive at decisions. This opacity undermines trust and accountability in critical domains such as healthcare, finance, and criminal justice.
                </p>
                </div>
            </section>

            <section class="parallax-wrapper5">
                <div class="stuff2">
                <h1 class="parallax-content">Job Automation</h1>
                <p class="parallax-content">
                    According to McKinsey, by 2030 up to 30% of work hours could be automated by AI. While new roles may emerge, many workers—particularly in undervalued sectors—may face displacement and struggle to re-skill, increasing inequality risks.  
                    (<a href="https://www.mckinsey.com/mgi/our-research/generative-ai-and-the-future-of-work-in-america" target="_blank">McKinsey forecast on automation</a>)
                </p>
                </div>
            </section>

            <section class="parallax-wrapper5">
                <div class="stuff2">
                <h1 class="parallax-content">Misinformation and Deepfakes</h1>
                <p class="parallax-content">
                    The rise of generative AI enables realistic synthetic media—deepfakes—that can misinform or manipulate audiences. Studies show they distort memories and trust, especially when deployed in political or social contexts.  
                    (<a href="https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0320124" target="_blank">PLOS One deepfake impact review</a>)
                </p>
                <p class="parallax-content">
                    Alarmingly, some deepfake detectors themselves exhibit racial bias—one study found up to a 10.7% error-rate disparity across demographic groups—highlighting fairness risks even in defensive technologies.  
                    (<a href="https://arxiv.org/abs/2105.00558" target="_blank">Deepfake detection fairness study</a>)
                </p>
                </div>
            </section>

            <section class="parallax-wrapper5">
                <div class="stuff2">
                <h1 class="parallax-content">Surveillance and Privacy Loss</h1>
                <p class="parallax-content">
                    AI-driven surveillance tools—such as facial recognition and predictive policing—often disproportionately target marginalized communities. In some cases, they've led to wrongful arrests of Black individuals due to algorithmic errors and insufficient oversight.  
                    (<a href="https://en.wikipedia.org/wiki/Anti-facial_recognition_movement" target="_blank">Facial recognition racial bias cases</a>)
                </p>
                <p class="parallax-content">
                    In one 2025 incident, a UK academic criticized a Metropolitan Police claim of "bias-free" facial recognition, pointing out the statistical underpinnings were flawed due to limited sample data.  
                    (<a href="https://www.theguardian.com/technology/2025/aug/23/expert-rejects-met-police-claim-that-study-backs-bias-free-live-facial-recognition-use" target="_blank">Guardian coverage on LFR criticism</a>)
                </p>
                </div>
            </section>

            <section class="parallax-wrapper5">
                <div class="stuff2">
                <h1 class="parallax-content">Mental Health and Social Well-Being</h1>
                <p class="parallax-content">
                    AI chatbots and recommendation systems optimized solely for engagement may foster addictive behavior, distort self-perception, or exacerbate mental health issues. Journalistic and early research accounts describe cases of "AI psychosis," where vulnerable users develop emotional over-dependence on AI companions.  
                    (<a href="https://www.washingtonpost.com/health/2025/08/19/ai-psychosis-chatgpt-explained-mental-health/" target="_blank">Washington Post on AI psychosis</a>)
                </p>
                </div>
            </section>



            <section id="solutions" class="break">
                <div id="content">
                    <h1 class="x-large">Solutions for Safer AI</h1>
                </div>
            </section>

            <section class="parallax-wrapper3">
                <div class="stuff2">
                <h1 class="parallax-content">Transparent Data Practices</h1>
                <p class="parallax-content">
                    Opening up datasets and model documentation promotes accountability, exposes biases, and fosters independent audits. Notable frameworks like the OECD’s "Tools for Trustworthy AI" help guide transparency practices for different use cases.  
                    (<a href="https://www.oecd.org/en/publications/tools-for-trustworthy-ai_008232ec-en.html" target="_blank">OECD transparency tools framework</a>)
                </p>
                </div>
            </section>

            <section class="parallax-wrapper3">
                <div class="stuff2">
                <h1 class="parallax-content">Explainable AI and Human Oversight</h1>
                <p class="parallax-content">
                    Incorporating human-in-the-loop mechanisms and explainable AI techniques—such as surrogate models or visual explanations—helps users comprehend and contest AI decisions, improving trust in high-stakes domains.
                </p>
                </div>
            </section>

            <section class="parallax-wrapper3">
                <div class="stuff2">
                <h1 class="parallax-content">Ethical Guidelines and Global Standards</h1>
                <p class="parallax-content">
                    The OECD AI Principles—established in 2019 and updated in 2024—frame trustworthy AI around five core values: inclusive growth, human rights, transparency, robustness, and accountability. The update added explicit protections for misinformation, environmental sustainability, and mechanisms to override harmful behavior in AI systems.  
                    (<a href="https://www.oecd.org/en/topics/ai-principles.html" target="_blank">OECD AI Principles overview</a>)  
                    (<a href="https://www.oecd.org/en/about/news/press-releases/2024/05/oecd-updates-ai-principles-to-stay-abreast-of-rapid-technological-developments.html" target="_blank">2024 updates to OECD AI Principles</a>)
                </p>
                </div>
            </section>

            <section class="parallax-wrapper3">
                <div class="stuff2">
                <h1 class="parallax-content">Worker Support and Reskilling</h1>
                <p class="parallax-content">
                    Proactive reskilling programs, lifelong learning initiatives, and policies encouraging human-AI collaboration can mitigate job displacement. Many OECD countries are already crafting education and social protection strategies rooted in these principles.  
                    (<a href="https://www.oecd.org/digital/artificial-intelligence/" target="_blank">OECD AI and social protection policies</a>)
                </p>
                </div>
            </section>

            <section class="parallax-wrapper3">
                <div class="stuff2">
                <h1 class="parallax-content">Inclusive and Participatory Design</h1>
                <p class="parallax-content">
                    Centering affected communities—such as workers, educators, and marginalized groups—in AI design ensures the technology addresses real needs and avoids unintended harm. For example, Joy Buolamwini’s Algorithmic Justice League pioneered participatory benchmarks and ethical pledges that influenced major AI provider practices.  
                    (<a href="https://en.wikipedia.org/wiki/Joy_Buolamwini" target="_blank">Joy Buolamwini’s AI equity efforts</a>)
                </p>
                </div>
            </section>


            <section id="unknown" class="break">
                <div id="content">
                    <h1 class="x-large">Unanswered Questions</h1>
                </div>
            </section>

            <section class="parallax-wrapper0">
                <div class="stuff2">
                <h1 class="parallax-content">How Should We Regulate AI?</h1>
                <p class="parallax-content">
                    Regulation must balance innovation with protection. Defining “AI” remains elusive—some simple algorithms are marketed as AI to attract attention. Meanwhile, global leaders like the Vatican urge regulation that ensures AI complements, rather than overrides, human values.  
                    (<a href="https://apnews.com/article/231b4b7b8ed6a195ec920f1f362c15e2" target="_blank">Vatican guidance on responsible AI</a>)
                </p>
                </div>
            </section>

            <section class="parallax-wrapper0">
                <div class="stuff2">
                <h1 class="parallax-content">Who Owns AI-Generated Content?</h1>
                <p class="parallax-content">
                    As AI generates art, text, and music, intellectual property frameworks are caught unprepared. Should dataset curators receive compensation? Is AI output public domain, corporate property, or a shared creation? Legal consensus is still forming globally.
                </p>
                </div>
            </section>

            <section class="parallax-wrapper0">
                <div class="stuff2">
                <h1 class="parallax-content">What Are the Long-Term Effects on Society?</h1>
                <p class="parallax-content">
                    Widespread AI use may erode human agency, critical thinking, and trust in shared reality—ushering in a “post-truth” environment amplified by deepfakes and algorithmic bubbles.  
                    (<a href="https://www.wired.com/story/generative-ai-deepfakes-disinformation-psychology/" target="_blank">Wired on AI and post-truth dynamics</a>)
                </p>
                </div>
            </section>
        </main>

        <footer id="footer">
            <div class="fcol1">
                <p>Credits:</p>
                <ul id="references">
                    <li>Image credits: Various sources via <a href="https://unsplash.com/" target="_blank">Unsplash</a></li>
                </ul>
            </div>
            <div class="fcol2">
                <p>© Website by <a href="https://mitchellpiehl.com" target="_blank" rel="noopener noreferrer">Mitchell Piehl</a>.</p>
            </div>
            <div class="fcol3">
                <a href="contact.html">Contact</a>
            </div>
        </footer>
    </div>
</body>
</html>
